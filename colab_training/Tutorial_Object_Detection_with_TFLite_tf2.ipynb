{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Tutorial_Object_Detection_with_TFLite.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "DS2GKozR1GLI"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/elephantscale/E2E-Object-Detection-in-TFLite/blob/master/colab_training/Tutorial_Object_Detection_with_TFLite_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aMB0P1OR1GLK"
   },
   "source": [
    "# Tutorial: Object Detection with TFLite\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Imagine that you have a niece or a nephew and you want to give them a present.\n",
    "When you were growing up, your aunt gave you a \"Find the Duck\" book. You had lots of fun\n",
    "finding the duck on every page of this board book. Today, you want to make this book\n",
    "into a computer game. For that, you need to be able to teach the computer how to find\n",
    "the duck. This is what this tutorial will teach you.\n",
    "\n",
    "<img src=\"find-the-duck-cartoon.png\"  alt=\"Find the duckie\" width=\"512\" height=\"512\">\n",
    "\n",
    "## Our plan\n",
    "\n",
    "The task that you are about to undertake is called \"Object Detection.\" The good news is that the\n",
    "Google library called TensorFlow already does most of the groundwork for object detection.\n",
    "Furthermore, the TensorFlow Lite part of the library will help you to put your application on a\n",
    "phone or a device app. The end result of your object detection will look like a screenshot below,\n",
    "where you will be able to detect, out of a known set of objects, which ones are present\n",
    "in our picture and what are their locations.\n",
    "\n",
    "<img src=\"object-detection-cartoon.png\"  alt=\"Find the fruit\" width=\"512\" height=\"512\">\n",
    "\n",
    "We will do it in three steps. First, you will have to prepare the data: those objects that you will be looking to identify.\n",
    "After you got the objects, you will have to convert them to TFrecord format that Object Detection API expects.\n",
    "Then, you will train the model with this data. And finally, you will export the model\n",
    "to TFLite, preparing it to be used in your phone app. In the next tutorial,\n",
    "we will teach you how to use the resulting TFLite model in your phone app. So, let us start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "P3HpM3Yd1GLM"
   },
   "source": [
    "## Data collection\n",
    "\n",
    "We have taken a dataset with pictures of fruit. Each data sample has an images\n",
    "of a fruit and an XML file with the fruit coordinates.\n",
    "This dataset came from Kaggle [here](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection), with a Creative Commons license,\n",
    "so for ease of use we have placed it in next to this tutorial.\n",
    "\n",
    "In Jupyter Notebook, you can do command lines if you only start with a exclamation sign.\n",
    "Let us download this dataset first."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "UeKKt1Iy1GLM"
   },
   "source": [
    "!wget -nc https://github.com/elephantscale/E2E-Object-Detection-in-TFLite/raw/master/data/Fruit_Images_for_Object_Detection.zip"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "mxMLlR7m1GLN"
   },
   "source": [
    "In the same way, let us unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6oFeghmD1GLN"
   },
   "source": [
    "!unzip -qqn Fruit_Images_for_Object_Detection.zip"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FG4nRJtL1GLO"
   },
   "source": [
    "## Generate intermediate files\n",
    "\n",
    "To be able to generate TFRecords from our fruits dataset we first generate a `.csv` file that would contain the following fields - \n",
    "- filename\n",
    "- width\n",
    "- height\n",
    "- class\n",
    "- xmin\n",
    "- ymin\n",
    "- xmax\n",
    "- ymax\n",
    "\n",
    "Now, we need to convert the XML descriptions to CSV. In your case,\n",
    "you may have to adjust this code, depending on the format of the data\n",
    "in your real-life dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hM4NeWw51GLO"
   },
   "source": [
    "# Convert XML to CSV\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def xml_to_csv(path):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     member[0].text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text)\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df\n",
    "\n",
    "\n",
    "def call_xml_to_csv():\n",
    "    train = \"/content/train_zip/train\"\n",
    "    test = \"/content/test_zip/test\"\n",
    "    for directory in [train, test]:\n",
    "        xml_df = xml_to_csv(directory)\n",
    "        xml_df.to_csv('{}_labels.csv'.format(directory), index=None)\n",
    "        print('Successfully converted xml to csv.')\n",
    "\n",
    "\n",
    "call_xml_to_csv()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "OjkiMFA_1GLP"
   },
   "source": [
    "!head -5 /content/train_zip/train_labels.csv"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "QskzCfEk1GLQ"
   },
   "source": [
    "!head -5 /content/test_zip/test_labels.csv"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "QQ0DUK-O1GLQ"
   },
   "source": [
    "Now that we have `.csv` files we can do some basic exploratory data analysis (EDA) to better understand the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "XyROFxos1GLQ"
   },
   "source": [
    "## Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ON3TvUXL1GLR"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "5uakkyN51GLR"
   },
   "source": [
    "train_df = pd.read_csv(\"/content/train_zip/train_labels.csv\")\n",
    "test_df = pd.read_csv(\"/content/test_zip/test_labels.csv\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "GjD45Euc1GLR"
   },
   "source": [
    "train_df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sFiRy4NN1GLS"
   },
   "source": [
    "test_df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "BD1LoWRC1GLS"
   },
   "source": [
    "train_df[\"class\"].value_counts()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "4_KDbgBy1GLS"
   },
   "source": [
    "test_df[\"class\"].value_counts()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "E0tEFAR81GLS"
   },
   "source": [
    "def show_images(df, is_train=True):\n",
    "    if is_train:\n",
    "        root = \"/content/train_zip/train\"\n",
    "    else:\n",
    "        root = \"/content/test_zip/test\"\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for i in range(10):\n",
    "        n = np.random.choice(df.shape[0], 1)\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(True)\n",
    "        image = plt.imread(os.path.join(root, df[\"filename\"][int(n)]))\n",
    "        plt.imshow(image)\n",
    "        label = df[\"class\"][int(n)]\n",
    "        plt.xlabel(label)\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PSoMtSb61GLT"
   },
   "source": [
    "show_images(train_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "2igCXIVz1GLT"
   },
   "source": [
    "show_images(test_df, is_train=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ZwY8u1jW1GLT"
   },
   "source": [
    "def verify_annotations(df, is_train=True):\n",
    "    if is_train:\n",
    "        root = \"/content/train_zip/train\"\n",
    "    else:\n",
    "        root = \"/content/test_zip/test\"\n",
    "    \n",
    "    plt.figure(figsize=(12,12))\n",
    "    for i in range(3):\n",
    "        n = np.random.choice(df.shape[0], 1)\n",
    "        plt.subplot(1,3,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "        image = plt.imread(os.path.join(root, df[\"filename\"][int(n)]))\n",
    "        xmin, ymin = int(df[\"xmin\"][int(n)]), int(df[\"ymin\"][int(n)])\n",
    "        xmax, ymax = int(df[\"xmax\"][int(n)]), int(df[\"ymax\"][int(n)])\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255,0,0), 3)\n",
    "        plt.imshow(image)\n",
    "    \n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "OOu9dvxX1GLU"
   },
   "source": [
    "verify_annotations(train_df, is_train=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ZFJSgmcH1GLV"
   },
   "source": [
    "verify_annotations(test_df, is_train=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Mrty9auJ1GLV"
   },
   "source": [
    "As we can see the dataset has annotation issues. So, our model training can suffer a lot from this. So, one can expect a model trained on this dataset might yield unexpected results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "K0KbxaOP1GLV"
   },
   "source": [
    "## Generate TFRecords and `.pbtxt`\n",
    "\n",
    "The TFRecord format is a simple format for storing a sequence of binary records.\n",
    "The format is explained [here](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n",
    "but for our purposes it is enough that the code below created these records for us.\n",
    "\n",
    "The utility scripts that I used in the following cells were adapted from [this repository](https://github.com/anirbankonar123/CorrosionDetector). \n",
    "\n",
    "TODO - should we convert to TF2 here?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "mVJvcQHL1GLW"
   },
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf \n",
    "print(tf.__version__)\n",
    "\n",
    "!git clone https://github.com/tensorflow/models.git\n",
    "\n",
    "% cd models/research\n",
    "!pip install --upgrade pip\n",
    "# Compile protos.\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "# Install TensorFlow Object Detection API.\n",
    "!cp object_detection/packages/tf1/setup.py .\n",
    "!python -m pip install --use-feature=2020-resolver ."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IMLiIL651GLW"
   },
   "source": [
    "#!wget https://raw.githubusercontent.com/elephantscale/E2E-Object-Detection-in-TFLite/master/colab_training/generate_tfrecord.py\n",
    "!wget https://raw.githubusercontent.com/elephantscale/E2E-Object-Detection-in-TFLite/tim/colab_training/generate_tfrecord.py\n",
    "!wget https://raw.githubusercontent.com/elephantscale/E2E-Object-Detection-in-TFLite/tim/colab_training/generate_tfrecord_tf2.py"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "S9pR8v7T1GLX"
   },
   "source": [
    "!python generate_tfrecord_tf2.py \\\n",
    "    --csv_input=/content/train_zip/train_labels.csv \\\n",
    "    --output_path=/content/train_zip/train.record"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "yVOyRRwa1GLX"
   },
   "source": [
    "Before the running the cell below please edit the `path` variable in the `main()` function of `generate_tfrecord.py`. `generate_tfrecord.py` should be located here - `/content/models/research`. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9L3nPiRO1GLX"
   },
   "source": [
    "!python generate_tfrecord_tf2.py \\\n",
    "    --csv_input=/content/test_zip/test_labels.csv \\\n",
    "    --output_path=/content/test_zip/test.record"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "tr3AuV2v1GLY"
   },
   "source": [
    "!pwd\n",
    "!ls -lh /content/test_zip/*.record\n",
    "!ls -lh /content/train_zip/*.record"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FOMGz_tB1GLY"
   },
   "source": [
    "Be sure to store these `.record` files to somewhere safe. Next, we need to generate a `.pbtxt` file that defines a mapping between our classes and integers. In the `generate_tfrecord.py` script, we used the following mapping - \n",
    "\n",
    "```python\n",
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'orange':\n",
    "        return 1\n",
    "    elif row_label == 'banana':\n",
    "        return 2\n",
    "    elif row_label == 'apple':\n",
    "        return 3\n",
    "    else:\n",
    "    \treturn None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_ZcJl1oE1GLY"
   },
   "source": [
    "label_encodings = {\n",
    "    \"orange\": 1,\n",
    "    \"banana\": 2,\n",
    "    \"apple\": 3\n",
    "}\n",
    "\n",
    "f = open(\"/content/label_map.pbtxt\", \"w\")\n",
    "\n",
    "for (k, v) in label_encodings.items():\n",
    "    item = (\"item {\\n\"\n",
    "            \"\\tid: \" + str(v) + \"\\n\"\n",
    "            \"\\tname: '\" + k + \"'\\n\"\n",
    "            \"}\\n\")\n",
    "    f.write(item)\n",
    "\n",
    "f.close()\n",
    "\n",
    "!cat /content/label_map.pbtxt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Jdjj37-q1GLY"
   },
   "source": [
    "Be sure to save this file as well. Next we will proceed toward training a custom detection model with what we have so far. Follow the steps in [this notebook](https://colab.research.google.com/github/sayakpaul/E2E-Object-Detection-in-TFLite/blob/master/colab_training/Training_MobileDet_Custom_Dataset.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ngb2n1lm1GLZ"
   },
   "source": [
    "In this notebook we will be fine-tuning a **MobileDet** model on the [**fruits dataset**](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection). The original model checkpoints were generated in TensorFlow 1, so we need to stick to a TF 1 runtime. The purpose is to demonstrate the workflow here and not achieve state-of-the-art results. So, please expect unexpected performance for a shorter training schedule. Toward the very end, we will also see how to optimize our fine-tuned model using TensorFlow Lite APIs and run inference with it. This part will be executed on a TF 2 runtime. \n",
    "\n",
    "As a prerequisite, you should be familiar with the contents of [this notebook](https://colab.research.google.com/github/sayakpaul/E2E-Object-Detection-in-TFLite/blob/master/colab_training/Fruits_Detection_Data_Prep.ipynb). It deals with the dataset contstruction part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aBLezDxs1GLZ"
   },
   "source": [
    "# Fetch pre-trained MobileDet model checkpoints and configuration\n",
    "\n",
    "MobileDet comes in different variants (refer [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md)). We will be using the `ssdlite_mobiledet_cpu` variant. \n",
    "\n",
    "TFOD API operates with configuration files to train and evaluate models (the TF 2 release supports eager model execution too). For the purpose of this notebook, I created a configuration file following instructions from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md). Note that I purposefully kept the `num_steps` argument to 2000. Here's a *non-exhaustive* list of the arguments I changed - \n",
    "\n",
    "-  `batch_size: 32`\n",
    "- `label_map_path` and `input_path` inside `train_input_reader` and `tf_record_input_reader` respectively. The `num_examples` argument inside `eval_config` is set to 117."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlp0CU9H3pwQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Download model checkpoint and config"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "QTjROKJY1GLa"
   },
   "source": [
    "!cd /content/models/research\n",
    "!wget -q http://download.tensorflow.org/models/object_detection/ssdlite_mobiledet_cpu_320x320_coco_2020_05_19.tar.gz\n",
    "!wget -q http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n",
    "!wget -q https://raw.githubusercontent.com/elephantscale/E2E-Object-Detection-in-TFLite/tim/colab_training/ssdlite_mobiledet_cpu_320x320_fruits_sync_4x4.config\n",
    "!wget -q https://raw.githubusercontent.com/elephantscale/E2E-Object-Detection-in-TFLite/tim/colab_training/ssdlite_mobiledet_cpu_320x320_fruits_sync_4x4_tf2.config"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ftmHMRB3znM"
   },
   "source": [
    "### Untar and verify the file structure of the model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0MxnnCKd1GLa"
   },
   "source": [
    "!cd /content/models/research\n",
    "!tar -xvf ssdlite_mobiledet_cpu_320x320_coco_2020_05_19.tar.gz\n",
    "!tar -xvf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n",
    "!cp /content/label_map.pbtxt /content/models/research\n",
    "!cp /content/test_zip/test.record /content/models/research\n",
    "!cp /content/train_zip/train.record /content/models/research"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Jvl8Bl4q1GLa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAlRR5uA2c8V",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Start training\n",
    "\n",
    "**Note**: This script interleaves both training and evaluation. Before starting the training verify the paths carefully.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qA0JPhAhl5LW"
   },
   "source": [
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] += \":/content/models\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/content/models\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "uPVIGCOl1GLb"
   },
   "source": [
    "#PIPELINE_CONFIG_PATH=\"/content/models/research/ssdlite_mobiledet_cpu_320x320_fruits_sync_4x4.config\"\n",
    "PIPELINE_CONFIG_PATH=\"/content/models/research/ssdlite_mobiledet_cpu_320x320_fruits_sync_4x4_tf2.config\"\n",
    "\n",
    "#MODEL_DIR=\"/content/models/research/ssdlite_mobiledet_cpu_320x320_coco_2020_05_19\"\n",
    "MODEL_DIR=\"/content/models/research/ssd_mobilenet_v2_320x320_coco17_tpu-8\"\n",
    "\n",
    "\n",
    "!python object_detection/model_main.py \\\n",
    "    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n",
    "    --model_dir={MODEL_DIR} \\\n",
    "    --alsologtostderr"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2ri-_3Ku1GLb"
   },
   "source": [
    "The above code block would take approximately **30 minutes** to run (although it depends on the GPU you got if you are running on Colab). If you increase the number of steps it would be even more. After the training was completed I got the following output - \n",
    "\n",
    "```\n",
    "I0915 04:48:33.129830 139851326252928 estimator.py:371] Loss for final step: 1.0553685.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ZaqW4_Vq1GLc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Export TFLite compatible graph\n",
    "\n",
    "To export the fine-tuned checkpoints to a TFLite model we first need to export a model graph that is compatible with TFLite. More instructions about this are available [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md). First, we need to determine which checkpoints to be used to export the graph. Let's first take a look at our `MODEL_DIR` to get an idea. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VZ0tiuH_1GLc"
   },
   "source": [
    "!ls -lh $MODEL_DIR"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8zu4YeIT1GLc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The checkpoint files with the prefix `model.ckpt-2000` are the ones we would be going with. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "fGXWVtWq1GLc"
   },
   "source": [
    "#Export TFLite compatible graph\n",
    "#Always verify the paths before running this command. \n",
    "\n",
    "!python object_detection/export_tflite_ssd_graph.py \\\n",
    "    --pipeline_config_path=$PIPELINE_CONFIG_PATH \\\n",
    "    --trained_checkpoint_prefix=$MODEL_DIR/model.ckpt-2000 \\\n",
    "    --output_directory=$MODEL_DIR \\\n",
    "    --add_postprocessing_op=true"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf0qz55u4WS7"
   },
   "source": [
    "### Verify the TFLite compatible graph size\n",
    "\n",
    " It should have the `.pb` extension. Be sure to note down the path you would get as the output of code block."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dNoI3AG11GLc"
   },
   "source": [
    "!ls -lh $MODEL_DIR/*.pb"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "nMW1ZANi1GLd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have the graph wcan convert it to TensorFlow Lite. Let's shift the runtime to TF 2. To do so, simply restart the Colab runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "XINsL0NX1GLd"
   },
   "source": [
    "# Optionally see the model losses in TensorBoard (within Colab Notebook)\n",
    "\n",
    "Note If you trained for 2000 steps only you are likely to see poor numbers in TensorBoard. But as I had mentioned training a SoTA model is not the purpose of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xG8nA66w1GLd"
   },
   "source": [
    "\n",
    "%tensorflow_version 2.x\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/models/research/ssdlite_mobiledet_cpu_320x320_coco_2020_05_19"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "hNcIC9JS1GLd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Export to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NoYm1p1D1GLe"
   },
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import os"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxXFYCiu4n-c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Quantize and serialize\n",
    "\n",
    "For the purpose of this notebook, we will only be quantizing using the [dynamic-range quantization](https://www.tensorflow.org/lite/performance/post_training_quant). But you can follow [this notebook](https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/MobileDet_Conversion_TFLite.ipynb) if you are interested to try out the other ones like integer quantization and `float16` quantization. \n",
    "\n",
    "As the .pb file we generated in the earlier step is a frozen graph, we need to use `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph` to convert it to TFLite.\n",
    "\n",
    "The MobileDet checkpoints we used accept 320x320 images, hence the `input_shapes` argument is specified that way. I specified the other arguments following instructions from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md).\n",
    "**bold text**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ezQFhESm1GLe"
   },
   "source": [
    "model_to_be_quantized = \"/content/models/research/ssdlite_mobiledet_cpu_320x320_coco_2020_05_19/tflite_graph.pb\"\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\n",
    "    graph_def_file=model_to_be_quantized, \n",
    "    input_arrays=['normalized_input_image_tensor'],\n",
    "    output_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'],\n",
    "    input_shapes={'normalized_input_image_tensor': [1, 320, 320, 3]}\n",
    ")\n",
    "converter.allow_custom_ops = True\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_filename = \"fruits_detector\" + \"_dr\" + \".tflite\"\n",
    "open(tflite_filename, 'wb').write(tflite_model)\n",
    "print(f\"TFLite model generated: {tflite_filename}\")\n",
    "!ls -lh $tflite_filename"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tnBfc_3s1GLe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "JKUrVErG1GLe"
   },
   "source": [
    "#Imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF_gkz1r49EH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TFLite Interpreter and detection utils \n",
    "\n",
    "Sourced from [here](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/raspberry_pi/detect_picamera.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "obPosiUw1GLf"
   },
   "source": [
    "\n",
    "def set_input_tensor(interpreter, image):\n",
    "  \"\"\"Sets the input tensor.\"\"\"\n",
    "  tensor_index = interpreter.get_input_details()[0]['index']\n",
    "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "  input_tensor[:, :] = image\n",
    "\n",
    "\n",
    "def get_output_tensor(interpreter, index):\n",
    "  \"\"\"Returns the output tensor at the given index.\"\"\"\n",
    "  output_details = interpreter.get_output_details()[index]\n",
    "  tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
    "  return tensor\n",
    "\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "  set_input_tensor(interpreter, image)\n",
    "  interpreter.invoke()\n",
    "\n",
    "  # Get all output details\n",
    "  boxes = get_output_tensor(interpreter, 0)\n",
    "  classes = get_output_tensor(interpreter, 1)\n",
    "  scores = get_output_tensor(interpreter, 2)\n",
    "  count = int(get_output_tensor(interpreter, 3))\n",
    "\n",
    "  results = []\n",
    "  for i in range(count):\n",
    "    if scores[i] >= threshold:\n",
    "      result = {\n",
    "          'bounding_box': boxes[i],\n",
    "          'class_id': classes[i],\n",
    "          'score': scores[i]\n",
    "      }\n",
    "      results.append(result)\n",
    "  return results"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "WH1VCEMI1GLf"
   },
   "source": [
    "# Supply a path to download a relevant image\n",
    "IMAGE_PATH = \"https://i.ibb.co/2tsXmCV/image.png\" \n",
    "\n",
    "!wget -q -O image.png $IMAGE_PATH\n",
    "Image.open('image.png')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "qeaUcBGB1GLf"
   },
   "source": [
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/content/fruits_detector_dr.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "_, HEIGHT, WIDTH, _ = interpreter.get_input_details()[0]['shape']\n",
    "print(f\"Height and width accepted by the model: {HEIGHT, WIDTH}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "2hc959mQrKbQ"
   },
   "source": [
    "# Image preprocessing utils\n",
    "def preprocess_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    original_image = img\n",
    "    resized_img = tf.image.resize(img, (HEIGHT, WIDTH))\n",
    "    resized_img = resized_img[tf.newaxis, :]\n",
    "    return resized_img, original_image"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "aGp6tBN2rKbR"
   },
   "source": [
    "# Define the label dictionary and color map\n",
    "LABEL_DICT = {\n",
    "    \"orange\": 1,\n",
    "    \"banana\": 2,\n",
    "    \"apple\": 3\n",
    "}\n",
    "\n",
    "REVERSE_LABEL_DICT = {\n",
    "    1 : \"orange\",\n",
    "    2 : \"banana\",\n",
    "    3 : \"apple\"\n",
    "}\n",
    "\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABEL_DICT), 3), \n",
    "                            dtype=\"uint8\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "GCEoQEDXrKbR"
   },
   "source": [
    "# Inference utils\n",
    "def display_results(image_path, threshold=0.3):\n",
    "    # Load the input image and preprocess it\n",
    "    preprocessed_image, original_image = preprocess_image(image_path)\n",
    "\n",
    "    # =============Perform inference=====================\n",
    "    start_time = time.monotonic()\n",
    "    results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
    "    print(f\"Elapsed time: {(time.monotonic() - start_time)*1000} miliseconds\")\n",
    "\n",
    "    # =============Display the results====================\n",
    "    original_numpy = original_image.numpy()\n",
    "    for obj in results:\n",
    "        # Convert the bounding box figures from relative coordinates\n",
    "        # to absolute coordinates based on the original resolution\n",
    "        ymin, xmin, ymax, xmax = obj['bounding_box']\n",
    "        xmin = int(xmin * original_numpy.shape[1])\n",
    "        xmax = int(xmax * original_numpy.shape[1])\n",
    "        ymin = int(ymin * original_numpy.shape[0])\n",
    "        ymax = int(ymax * original_numpy.shape[0])\n",
    "\n",
    "        # Grab the class index for the current iteration\n",
    "        idx = int(obj['class_id'])\n",
    "        # Skip the background\n",
    "        if idx >= len(LABEL_DICT):\n",
    "            continue\n",
    "\n",
    "        # Draw the bounding box and label on the image\n",
    "        color = [int(c) for c in COLORS[idx]]\n",
    "        cv2.rectangle(original_numpy, (xmin, ymin), (xmax, ymax), \n",
    "                    color, 2)\n",
    "        y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "        label = \"{}: {:.2f}%\".format(REVERSE_LABEL_DICT[idx],\n",
    "            obj['score'] * 100)\n",
    "        cv2.putText(original_numpy, label, (xmin, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # return the final imaage\n",
    "    original_int = (original_numpy * 255).astype(np.uint8)\n",
    "    return original_int"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "adSuWchyrKbR"
   },
   "source": [
    "# Run inference and measure the inference time\n",
    "resultant_image = display_results(\"/content/image.png\", threshold=0.3)\n",
    "Image.fromarray(resultant_image)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "YJvGnAn21GLg"
   },
   "source": [
    "**Note** that you might see some unexpected results because the annotations in the training dataset are faulty at places. Due to this the model training can suffer a lot. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "pbK8UjqX1GLg"
   },
   "source": [
    "# Define the label dictionary and color map\n",
    "LABEL_DICT = {\n",
    "    \"orange\": 1,\n",
    "    \"banana\": 2,\n",
    "    \"apple\": 3\n",
    "}\n",
    "\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABEL_DICT), 3), \n",
    "                            dtype=\"uint8\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "g_0AJAQP1GLg"
   },
   "source": [
    "# Inference utils\n",
    "def display_results(image_path, threshold=0.3):\n",
    "    # Load the input image and preprocess it\n",
    "    preprocessed_image, original_image = preprocess_image(image_path)\n",
    "\n",
    "    # =============Perform inference=====================\n",
    "    start_time = time.monotonic()\n",
    "    results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
    "    print(f\"Elapsed time: {(time.monotonic() - start_time)*1000} miliseconds\")\n",
    "\n",
    "    # =============Display the results====================\n",
    "    original_numpy = original_image.numpy()\n",
    "    for obj in results:\n",
    "        # Convert the bounding box figures from relative coordinates\n",
    "        # to absolute coordinates based on the original resolution\n",
    "        ymin, xmin, ymax, xmax = obj['bounding_box']\n",
    "        xmin = int(xmin * original_numpy.shape[1])\n",
    "        xmax = int(xmax * original_numpy.shape[1])\n",
    "        ymin = int(ymin * original_numpy.shape[0])\n",
    "        ymax = int(ymax * original_numpy.shape[0])\n",
    "\n",
    "        # Grab the class index for the current iteration\n",
    "        idx = int(obj['class_id'])\n",
    "        # Skip the background\n",
    "        if idx >= len(LABEL_DICT):\n",
    "            continue\n",
    "\n",
    "        # Draw the bounding box and label on the image\n",
    "        color = [int(c) for c in COLORS[idx]]\n",
    "        cv2.rectangle(original_numpy, (xmin, ymin), (xmax, ymax), \n",
    "                    color, 2)\n",
    "        y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "        label = \"{}: {:.2f}%\".format(REVERSE_LABEL_DICT[idx],\n",
    "            obj['score'] * 100)\n",
    "        cv2.putText(original_numpy, label, (xmin, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # return the final imaage\n",
    "    original_int = (original_numpy * 255).astype(np.uint8)\n",
    "    return original_int"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "72wx_SlN1GLh"
   },
   "source": [
    "# Run inference and measure the inference time\n",
    "resultant_image = display_results(\"/content/image.png\", threshold=0.3)\n",
    "Image.fromarray(resultant_image)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "KhryxYtL1GLh"
   },
   "source": [
    "**Note** that you might see some unexpected results because the annotations in the training dataset are faulty at places. Due to this the model training can suffer a lot. "
   ]
  }
 ]
}
