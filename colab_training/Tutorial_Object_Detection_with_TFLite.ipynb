{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "new_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elephantscale/E2E-Object-Detection-in-TFLite/blob/master/colab_training/Tutorial_Object_Detection_with_TFLite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyYfJF3yUhw7"
      },
      "source": [
        "# Tutorial: Object Detection with TFLite\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Imagine that you have a niece or a nephew and you want to give them a present.\n",
        "When you were growing up, your ant gave you a \"Find the Duck\" book. You had lots of fun\n",
        "finding the duck on every page of this board book. Today, you want to make this book\n",
        "into a computer game. For that, you need to be able to teach the computer how to find\n",
        "the duck. This is what this tutorial will teach you.\n",
        "\n",
        "<img src=\"find-the-duck-50.png\"/>\n",
        "\n",
        "## Our plan\n",
        "\n",
        "The task that you are about to undertake is called \"Object Detection.\" The good news is that the\n",
        "Google library called TensorFlow already does most of the groundwork for object detection.\n",
        "Furthermore, the TensorFlow Lite part of the library will help you to put your application on a\n",
        "phone or a device app. The end result of your object detection will look like a screenshot below,\n",
        "where you will be able to detect, out of a known set of objects, which ones are present\n",
        "in our picture and what are their locations.\n",
        "\n",
        "<img src=\"object-detection.png\"/>\n",
        "\n",
        "We will do it in three steps. First, you will have to prepare the data: those objects that you will be looking to identify.\n",
        "After you got the objects, you will have to convert them to TFrecord format that Object Detection API expects.\n",
        "Then, you will train the model with this data. And finally, you will export the model\n",
        "to TFLite, preparing it to be used in your phone app. In the next tutorial,\n",
        "we will teach you how to use the resulting TFLite model in your phone app. So, let us start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ynDtloV1DyJ"
      },
      "source": [
        "## Data collection\n",
        "\n",
        "Dataset homepage: https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd1bUjXKhkiB"
      },
      "source": [
        "!wget -nc https://github.com/elephantscale/E2E-Object-Detection-in-TFLite/raw/master/data/Fruit_Images_for_Object_Detection.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr8o4K8NjhQu"
      },
      "source": [
        "!unzip -qqn Fruit_Images_for_Object_Detection.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyFfc8-T1dIr"
      },
      "source": [
        "## Generate intermediate files\n",
        "\n",
        "To be able to generate TFRecords from our fruits dataset we first generate a `.csv` file that would contain the following fields - \n",
        "- filename\n",
        "- width\n",
        "- height\n",
        "- class\n",
        "- xmin\n",
        "- ymin\n",
        "- xmax\n",
        "- ymax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYA1k1qthhZD"
      },
      "source": [
        "# Convert XML to CSV\n",
        "# Originally sourced from https://github.com/anirbankonar123/CorrosionDetector/blob/master/xml_to_csv.py\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "def xml_to_csv(path):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(member[4][0].text),\n",
        "                     int(member[4][1].text),\n",
        "                     int(member[4][2].text),\n",
        "                     int(member[4][3].text)\n",
        "                     )\n",
        "            xml_list.append(value)\n",
        "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    return xml_df\n",
        "\n",
        "\n",
        "def call_xml_to_csv():\n",
        "    train = \"/content/train_zip/train\"\n",
        "    test = \"/content/test_zip/test\"\n",
        "    for directory in [train, test]:\n",
        "        xml_df = xml_to_csv(directory)\n",
        "        xml_df.to_csv('{}_labels.csv'.format(directory), index=None)\n",
        "        print('Successfully converted xml to csv.')\n",
        "\n",
        "\n",
        "call_xml_to_csv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpQZmcTglsLy"
      },
      "source": [
        "!head -5 /content/train_zip/train_labels.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg7kvco7mZLA"
      },
      "source": [
        "!head -5 /content/test_zip/test_labels.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb9BwGlR19Ww"
      },
      "source": [
        "Now that we have `.csv` files we can do some basic exploratory data analysis (EDA) to better understand the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr3tSXiCoK36"
      },
      "source": [
        "## Basic EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTJfUh2pm4c0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3URS71koW1H"
      },
      "source": [
        "train_df = pd.read_csv(\"/content/train_zip/train_labels.csv\")\n",
        "test_df = pd.read_csv(\"/content/test_zip/test_labels.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtBt_AYjogIE"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsIwA7wMohwe"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvdTLpmhojKM"
      },
      "source": [
        "train_df[\"class\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4lMIKB_oncV"
      },
      "source": [
        "test_df[\"class\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSSRljJMoqZ9"
      },
      "source": [
        "def show_images(df, is_train=True):\n",
        "    if is_train:\n",
        "        root = \"/content/train_zip/train\"\n",
        "    else:\n",
        "        root = \"/content/test_zip/test\"\n",
        "    plt.figure(figsize=(15,15))\n",
        "    for i in range(10):\n",
        "        n = np.random.choice(df.shape[0], 1)\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(True)\n",
        "        image = plt.imread(os.path.join(root, df[\"filename\"][int(n)]))\n",
        "        plt.imshow(image)\n",
        "        label = df[\"class\"][int(n)]\n",
        "        plt.xlabel(label)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGcjO2ripKdR"
      },
      "source": [
        "show_images(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1Di0_rOpMV0"
      },
      "source": [
        "show_images(test_df, is_train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc-TTag0pp5q"
      },
      "source": [
        "def verify_annotations(df, is_train=True):\n",
        "    if is_train:\n",
        "        root = \"/content/train_zip/train\"\n",
        "    else:\n",
        "        root = \"/content/test_zip/test\"\n",
        "    \n",
        "    plt.figure(figsize=(12,12))\n",
        "    for i in range(3):\n",
        "        n = np.random.choice(df.shape[0], 1)\n",
        "        plt.subplot(1,3,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        \n",
        "        image = plt.imread(os.path.join(root, df[\"filename\"][int(n)]))\n",
        "        xmin, ymin = int(df[\"xmin\"][int(n)]), int(df[\"ymin\"][int(n)])\n",
        "        xmax, ymax = int(df[\"xmax\"][int(n)]), int(df[\"ymax\"][int(n)])\n",
        "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255,0,0), 3)\n",
        "        plt.imshow(image)\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3ZMF5w0rAS-"
      },
      "source": [
        "verify_annotations(train_df, is_train=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTky5iU9rHpT"
      },
      "source": [
        "verify_annotations(test_df, is_train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HrBfhYfeNjt"
      },
      "source": [
        "As we can see the dataset has annotation issues. So, our model training can suffer a lot from this. So, one can expect a model trained on this dataset might yield unexpected results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5-TguVVzNJd"
      },
      "source": [
        "## Generate TFRecords and `.pbtxt`\n",
        "\n",
        "Explaining the steps of creating TFRecords is out of scope here. Please follow this Kaggle kernel that sheds some light on the process. \n",
        "\n",
        "The utility scripts that I used in the following cells were adapted from [this repository](https://github.com/anirbankonar123/CorrosionDetector). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c89IFm7se0ug"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aT-AJsosHZq"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf \n",
        "print(tf.__version__)\n",
        "\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "\n",
        "% cd models/research\n",
        "!pip install --upgrade pip\n",
        "# Compile protos.\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "# Install TensorFlow Object Detection API.\n",
        "!cp object_detection/packages/tf1/setup.py .\n",
        "!python -m pip install --use-feature=2020-resolver ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umlUy7GOtFRZ"
      },
      "source": [
        "#!wget https://raw.githubusercontent.com/elephantscale/E2E-Object-Detection-in-TFLite/master/colab_training/generate_tfrecord.py\n",
        "!wget https://raw.githubusercontent.com/elephantscale/E2E-Object-Detection-in-TFLite/tim/colab_training/generate_tfrecord.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijp-_gvswPjv"
      },
      "source": [
        "!python generate_tfrecord.py \\\n",
        "    --csv_input=/content/train_zip/train_labels.csv \\\n",
        "    --output_path=/content/train_zip/train.record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTp_KcrXxgON"
      },
      "source": [
        "Before the running the cell below please edit the `path` variable in the `main()` function of `generate_tfrecord.py`. `generate_tfrecord.py` should be located here - `/content/models/research`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5I_eRHtwigV"
      },
      "source": [
        "!python generate_tfrecord.py \\\n",
        "    --csv_input=/content/test_zip/test_labels.csv \\\n",
        "    --output_path=/content/test_zip/test.record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbRvRmDBx9NC"
      },
      "source": [
        "!pwd\n",
        "!ls -lh /content/test_zip/*.record\n",
        "!ls -lh /content/train_zip/*.record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LNIRZQizwwu"
      },
      "source": [
        "Be sure to store these `.record` files to somewhere safe. Next, we need to generate a `.pbtxt` file that defines a mapping between our classes and integers. In the `generate_tfrecord.py` script, we used the following mapping - \n",
        "\n",
        "```python\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'orange':\n",
        "        return 1\n",
        "    elif row_label == 'banana':\n",
        "        return 2\n",
        "    elif row_label == 'apple':\n",
        "        return 3\n",
        "    else:\n",
        "    \treturn None\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiveLk4m0Gbb"
      },
      "source": [
        "label_encodings = {\n",
        "    \"orange\": 1,\n",
        "    \"banana\": 2,\n",
        "    \"apple\": 3\n",
        "}\n",
        "\n",
        "f = open(\"/content/label_map.pbtxt\", \"w\")\n",
        "\n",
        "for (k, v) in label_encodings.items():\n",
        "    item = (\"item {\\n\"\n",
        "            \"\\tid: \" + str(v) + \"\\n\"\n",
        "            \"\\tname: '\" + k + \"'\\n\"\n",
        "            \"}\\n\")\n",
        "    f.write(item)\n",
        "\n",
        "f.close()\n",
        "\n",
        "!cat /content/label_map.pbtxt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M29rUmj0wKb"
      },
      "source": [
        "Be sure to save this file as well. Next we will proceed toward training a custom detection model with what we have so far. Follow the steps in [this notebook](https://colab.research.google.com/github/sayakpaul/E2E-Object-Detection-in-TFLite/blob/master/colab_training/Training_MobileDet_Custom_Dataset.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXY-mBNDAXXQ"
      },
      "source": [
        "In this notebook we will be fine-tuning a **MobileDet** model on the [**fruits dataset**](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection). The original model checkpoints were generated in TensorFlow 1, so we need to stick to a TF 1 runtime. The purpose is to demonstrate the workflow here and not achieve state-of-the-art results. So, please expect unexpected performance for a shorter training schedule. Toward the very end, we will also see how to optimize our fine-tuned model using TensorFlow Lite APIs and run inference with it. This part will be executed on a TF 2 runtime. \n",
        "\n",
        "As a prerequisite, you should be familiar with the contents of [this notebook](https://colab.research.google.com/github/sayakpaul/E2E-Object-Detection-in-TFLite/blob/master/colab_training/Fruits_Detection_Data_Prep.ipynb). It deals with the dataset contstruction part. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TD8j2hYB1Xt"
      },
      "source": [
        "# Fetch pre-trained MobileDet model checkpoints and configuration\n",
        "\n",
        "MobileDet comes in different variants (refer [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md)). We will be using the `ssdlite_mobiledet_cpu` variant. \n",
        "\n",
        "TFOD API operates with configuration files to train and evaluate models (the TF 2 release supports eager model execution too). For the purpose of this notebook, I created a configuration file following instructions from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md). Note that I purposefully kept the `num_steps` argument to 2000. Here's a *non-exhaustive* list of the arguments I changed - \n",
        "\n",
        "-  `batch_size: 32`\n",
        "- `label_map_path` and `input_path` inside `train_input_reader` and `tf_record_input_reader` respectively. The `num_examples` argument inside `eval_config` is set to 117."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrEVnZ9u0e0Y"
      },
      "source": [
        "#@title Download model checkpoint and config\n",
        "!cd /content/models/research\n",
        "!wget -q http://download.tensorflow.org/models/object_detection/ssdlite_mobiledet_cpu_320x320_coco_2020_05_19.tar.gz\n",
        "!wget -q https://gist.githubusercontent.com/sayakpaul/9efad54dee957cc55b3adacf992a7a47/raw/7ab5cf7db9e7f7d797d06d15c8b50ace93413643/ssdlite_mobiledet_cpu_320x320_fruits_sync_4x4.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U63CG0sE3bra"
      },
      "source": [
        "#@title Untar and verify the file structure of the model checkpoints\n",
        "!cd /content/models/research\n",
        "!tar -xvf ssdlite_mobiledet_cpu_320x320_coco_2020_05_19.tar.gz\n",
        "!cd /content/models/research\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QkB3E3YEOmT"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKz5DvQc31SW"
      },
      "source": [
        "!ls /content/models/research/\n",
        "!find /content -name \"ssd*\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMICifSU56lG"
      },
      "source": [
        "!ls /content/models/research"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VautcqJF4ALz"
      },
      "source": [
        "#@title Start training\n",
        "#@markdown **Note**: This script interleaves both training and evaluation. Before starting the training verify the paths carefully. \n",
        "PIPELINE_CONFIG_PATH=\"/content/models/research/models/research/ssdlite_mobiledet_cpu_320x320_fruits_sync_4x4.config\"\n",
        "MODEL_DIR=\"/content/models/research/models/research/ssdlite_mobiledet_cpu_320x320_coco_2020_05_19\"\n",
        "\n",
        "!python object_detection/model_main.py \\\n",
        "    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n",
        "    --model_dir={MODEL_DIR} \\\n",
        "    --alsologtostderr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-mkklBXWkBK"
      },
      "source": [
        "The above code block would take approximately **30 minutes** to run (although it depends on the GPU you got if you are running on Colab). If you increase the number of steps it would be even more. After the training was completed I got the following output - \n",
        "\n",
        "```\n",
        "I0915 04:48:33.129830 139851326252928 estimator.py:371] Loss for final step: 1.0553685.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7_KRkL3E7gX"
      },
      "source": [
        "# Export TFLite compatible graph\n",
        "\n",
        "To export the fine-tuned checkpoints to a TFLite model we first need to export a model graph that is compatible with TFLite. More instructions about this are available [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md). First, we need to determine which checkpoints to be used to export the graph. Let's first take a look at our `MODEL_DIR` to get an idea. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miAc302CbBN_"
      },
      "source": [
        "!ls -lh $MODEL_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-c0KP1Nbcqy"
      },
      "source": [
        "The checkpoint files with the prefix `model.ckpt-2000` are the ones we would be going with. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vzNWyy6oFP0u"
      },
      "source": [
        "#@title Export TFLite compatible graph\n",
        "#@markdown Always verify the paths before running this command. \n",
        "!python object_detection/export_tflite_ssd_graph.py \\\n",
        "    --pipeline_config_path=$PIPELINE_CONFIG_PATH \\\n",
        "    --trained_checkpoint_prefix=$MODEL_DIR/model.ckpt-2000 \\\n",
        "    --output_directory=$MODEL_DIR \\\n",
        "    --add_postprocessing_op=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Mq0-lXRycoxg"
      },
      "source": [
        "#@title Verify the TFLite compatible graph size\n",
        "#@markdown It should have the `.pb` extension. Be sure to note down the path you would get as the output of code block.\n",
        "!ls -lh $MODEL_DIR/*.pb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhG1m_-yFvZK"
      },
      "source": [
        "Now that we have the graph wcan convert it to TensorFlow Lite. Let's shift the runtime to TF 2. To do so, simply restart the Colab runtime. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR_1AbAXc8qU"
      },
      "source": [
        "# Optionally see the model losses in TensorBoard (within Colab Notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "iS5kFPYDdEqb"
      },
      "source": [
        "#@title Note\n",
        "#@markdown If you trained for 2000 steps only you are likely to see poor numbers in TensorBoard. But as I had mentioned training a SoTA model is not the purpose of this notebook. \n",
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/models/research/ssdlite_mobiledet_cpu_320x320_coco_2020_05_19"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnpRfgbUUESf"
      },
      "source": [
        "# Export to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "kk14y9a4d_Xn"
      },
      "source": [
        "#@title Imports\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gXfEn_ILeP0c"
      },
      "source": [
        "#@title Quantize and serialize\n",
        "#@markdown For the purpose of this notebook, we will only be quantizing using the [dynamic-range quantization](https://www.tensorflow.org/lite/performance/post_training_quant). But you can follow [this notebook](https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/MobileDet_Conversion_TFLite.ipynb) if you are interested to try out the other ones like integer quantization and `float16` quantization. \n",
        "\n",
        "#@markdown As the .pb file we generated in the earlier step is a frozen graph, we need to use `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph` to convert it to TFLite.\n",
        "\n",
        "#@markdown The MobileDet checkpoints we used accept 320x320 images, hence the `input_shapes` argument is specified that way. I specified the other arguments following instructions from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md).\n",
        "model_to_be_quantized = \"/content/models/research/ssdlite_mobiledet_cpu_320x320_coco_2020_05_19/tflite_graph.pb\"\n",
        "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\n",
        "    graph_def_file=model_to_be_quantized, \n",
        "    input_arrays=['normalized_input_image_tensor'],\n",
        "    output_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'],\n",
        "    input_shapes={'normalized_input_image_tensor': [1, 320, 320, 3]}\n",
        ")\n",
        "converter.allow_custom_ops = True\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_filename = \"fruits_detector\" + \"_dr\" + \".tflite\"\n",
        "open(tflite_filename, 'wb').write(tflite_model)\n",
        "print(f\"TFLite model generated: {tflite_filename}\")\n",
        "!ls -lh $tflite_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73skE65OUGpi"
      },
      "source": [
        "# Run inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6HWYNMxYgg5z"
      },
      "source": [
        "#@title Imports\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Cf9X-JGZgik3"
      },
      "source": [
        "#@title TFLite Interpreter and detection utils \n",
        "#@markdown Sourced from [here](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/raspberry_pi/detect_picamera.py).\n",
        "\n",
        "def set_input_tensor(interpreter, image):\n",
        "  \"\"\"Sets the input tensor.\"\"\"\n",
        "  tensor_index = interpreter.get_input_details()[0]['index']\n",
        "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
        "  input_tensor[:, :] = image\n",
        "\n",
        "\n",
        "def get_output_tensor(interpreter, index):\n",
        "  \"\"\"Returns the output tensor at the given index.\"\"\"\n",
        "  output_details = interpreter.get_output_details()[index]\n",
        "  tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
        "  return tensor\n",
        "\n",
        "\n",
        "def detect_objects(interpreter, image, threshold):\n",
        "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
        "  set_input_tensor(interpreter, image)\n",
        "  interpreter.invoke()\n",
        "\n",
        "  # Get all output details\n",
        "  boxes = get_output_tensor(interpreter, 0)\n",
        "  classes = get_output_tensor(interpreter, 1)\n",
        "  scores = get_output_tensor(interpreter, 2)\n",
        "  count = int(get_output_tensor(interpreter, 3))\n",
        "\n",
        "  results = []\n",
        "  for i in range(count):\n",
        "    if scores[i] >= threshold:\n",
        "      result = {\n",
        "          'bounding_box': boxes[i],\n",
        "          'class_id': classes[i],\n",
        "          'score': scores[i]\n",
        "      }\n",
        "      results.append(result)\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "iOZORwKsgt_2"
      },
      "source": [
        "#@title Supply a path to download a relevant image\n",
        "IMAGE_PATH = \"https://i.ibb.co/2tsXmCV/image.png\" #@param {type:\"string\"}\n",
        "\n",
        "!wget -q -O image.png $IMAGE_PATH\n",
        "Image.open('image.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ZJRxm65EgweI"
      },
      "source": [
        "#@title Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/fruits_detector_dr.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "_, HEIGHT, WIDTH, _ = interpreter.get_input_details()[0]['shape']\n",
        "print(f\"Height and width accepted by the model: {HEIGHT, WIDTH}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2B_G0jEog3hj"
      },
      "source": [
        "#@title Image preprocessing utils\n",
        "def preprocess_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_image(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    original_image = img\n",
        "    resized_img = tf.image.resize(img, (HEIGHT, WIDTH))\n",
        "    resized_img = resized_img[tf.newaxis, :]\n",
        "    return resized_img, original_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KDQoSAf0g555"
      },
      "source": [
        "#@title Define the label dictionary and color map\n",
        "LABEL_DICT = {\n",
        "    \"orange\": 1,\n",
        "    \"banana\": 2,\n",
        "    \"apple\": 3\n",
        "}\n",
        "\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABEL_DICT), 3), \n",
        "                            dtype=\"uint8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "xwW7ZJdOg-Eu"
      },
      "source": [
        "#@title Inference utils\n",
        "def display_results(image_path, threshold=0.3):\n",
        "    # Load the input image and preprocess it\n",
        "    preprocessed_image, original_image = preprocess_image(image_path)\n",
        "\n",
        "    # =============Perform inference=====================\n",
        "    start_time = time.monotonic()\n",
        "    results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
        "    print(f\"Elapsed time: {(time.monotonic() - start_time)*1000} miliseconds\")\n",
        "\n",
        "    # =============Display the results====================\n",
        "    original_numpy = original_image.numpy()\n",
        "    for obj in results:\n",
        "        # Convert the bounding box figures from relative coordinates\n",
        "        # to absolute coordinates based on the original resolution\n",
        "        ymin, xmin, ymax, xmax = obj['bounding_box']\n",
        "        xmin = int(xmin * original_numpy.shape[1])\n",
        "        xmax = int(xmax * original_numpy.shape[1])\n",
        "        ymin = int(ymin * original_numpy.shape[0])\n",
        "        ymax = int(ymax * original_numpy.shape[0])\n",
        "\n",
        "        # Grab the class index for the current iteration\n",
        "        idx = int(obj['class_id'])\n",
        "        # Skip the background\n",
        "        if idx >= len(LABEL_DICT):\n",
        "            continue\n",
        "\n",
        "        # Draw the bounding box and label on the image\n",
        "        color = [int(c) for c in COLORS[idx]]\n",
        "        cv2.rectangle(original_numpy, (xmin, ymin), (xmax, ymax), \n",
        "                    color, 2)\n",
        "        y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
        "        label = \"{}: {:.2f}%\".format(LABEL_DICT[idx],\n",
        "            obj['score'] * 100)\n",
        "        cv2.putText(original_numpy, label, (xmin, y),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    # return the final imaage\n",
        "    original_int = (original_numpy * 255).astype(np.uint8)\n",
        "    return original_int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "4xixRWWdhGGS"
      },
      "source": [
        "#@title Run inference and measure the inference time\n",
        "resultant_image = display_results(\"/content/image.png\", threshold=0.3)\n",
        "Image.fromarray(resultant_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vRzPBK2NJsM"
      },
      "source": [
        "**Note** that you might see some unexpected results because the annotations in the training dataset are faulty at places. Due to this the model training can suffer a lot. "
      ]
    }
  ]
}